{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d080cd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from typing import Any, Dict, List\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "41367aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time_str = \"2025-04-28_16-08-34\"\n",
    "\n",
    "# Folder with the llm annotations\n",
    "LLM_ANNOTATIONS = f\"../llm_outputs/annotations_{date_time_str}/\"\n",
    "\n",
    "# List of entities to tag (by the llms) and then extract\n",
    "TAGS = [\"MOL\", \"SOFTNAME\", \"SOFTVERS\", \"STIME\", \"TEMP\", \"FFM\"]\n",
    "\n",
    "# Path to the where we will be writing the quality control results\n",
    "QC_RESULTS_PATH = f\"../llm_outputs/stats_{date_time_str}/quality_control_results.csv\"\n",
    "QC_RESULTS_FOLDER = f\"../llm_outputs/stats_{date_time_str}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e48ccb",
   "metadata": {},
   "source": [
    "## **Quality control**\n",
    "---\n",
    "\n",
    "Before scoring the LLM responses, we implement a few functions to perform a basic quality check. This check helps ensure that the outputs meet a minimum standard before moving forward with evaluation or further processing.\n",
    "\n",
    "For now, our quality control focuses on:\n",
    "1. verifying that the LLM response includes the original input text\n",
    "2. checking if at least one entitiy found by the llm exists in the original text. \n",
    "\n",
    "\n",
    "This simple check helps catch hallucinations or unrelated outputs from the model. Note that the comparison is **case-insensitive**.\n",
    "\n",
    "This is by no means a filter to the responses, but rather will help up filter later on, but also will give help us give an idea of the degree of liberty that the LLM takes to answer our query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc982de",
   "metadata": {},
   "source": [
    "To ensure the integrity of the LLM annotations, we need to verify that the output text closely matches the input text we provided.\n",
    "\n",
    "1. **Strip annotation tags**\n",
    "\n",
    "Since the LLM response includes XML-like tags (e.g., `<TAG LABEL>` and `</TAG LABEL>`), we first remove these tags to isolate the raw text. This allows for a fair comparison between the original input and the annotated output.\n",
    "\n",
    "2. **Compare input and output texts**\n",
    "\n",
    "Once tags are stripped, we compare the cleaned LLM output to the original input text. This helps us confirm that the model has not introduced hallucinated content or omitted any part of the input. Only the responses that **pass this check** are retained for further analysis or evaluation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b6e4a18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to strip tags from the annotated text\n",
    "def strip_tags(text:str, tags=TAGS) -> str:\n",
    "    \"\"\"\n",
    "    Removes the custom tags from the annotated text.\n",
    "    \"\"\"\n",
    "    for tag in tags:\n",
    "        text = re.sub(f\"</?{re.escape(tag)}>\", \"\", text)\n",
    "    return text.strip()\n",
    "\n",
    "# Function to compare the annotated text to the original input\n",
    "def compare_annotated_to_original(original: str, annotated: str) -> bool:\n",
    "    \"\"\"\n",
    "    Compares tag-stripped annotated text to the original input in lowercase.\n",
    "    Returns True if they match exactly (ignoring case), False otherwise.\n",
    "    \"\"\"\n",
    "    stripped = strip_tags(annotated).strip().lower()\n",
    "    original = original.strip().lower()\n",
    "\n",
    "    return stripped == original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648d4d38",
   "metadata": {},
   "source": [
    "3. **Check if at least one entity found, is present in the original text**\n",
    "\n",
    "For this step, we need to first of all load the llm json file in order to be able to read and analyse the model response. From the LLM response, we are going to need to extract the entities found in the XML-like tags. Then we go through the entities, and search to find if at least one entity is present in the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f2e4527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_llm_json_file(json_file: str) -> tuple:\n",
    "    with open(json_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Extract the input text, response, and model\n",
    "    text_to_annotate = data[\"text_to_annotate\"]\n",
    "    response = data[\"response\"]\n",
    "    model = data[\"model\"]\n",
    "\n",
    "    return text_to_annotate, response, model\n",
    "\n",
    "def extract_entities_from_llm_text(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extract entities from an output text based on tagged annotations.\n",
    "    \n",
    "    The input text is expected to have entities enclosed in tags, e.g.:\n",
    "    \"Extending the Stochastic Titration CpHMD to <FFM>CHARMM36m</FFM> using <SOFTNAME>Gromacs</SOFTNAME>\"\n",
    "    \n",
    "    The function returns a dictionary with keys corresponding to the desired entity types\n",
    "    and values as lists with the extracted entity content.\n",
    "    \"\"\"\n",
    "    # Initialize the results with empty lists for all desired keys.\n",
    "    result = {key: [] for key in TAGS}\n",
    "    \n",
    "    # Use a regex to capture tags in the format <TAG>content</TAG>\n",
    "    # The regex uses a backreference to ensure matching closing tag.\n",
    "    pattern = re.compile(r\"<([A-Z]+)>(.*?)</\\1>\")\n",
    "    \n",
    "    # Find all matches in the text.\n",
    "    for tag, content in pattern.findall(text):\n",
    "        # If the tag is one of our desired keys, append the content (stripped of whitespace)\n",
    "        if tag in result:\n",
    "            result[tag].append(content.strip())\n",
    "    \n",
    "    return result\n",
    "\n",
    "def find_one_valid_llm_entity(llm_entities:dict, input_text:str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if at least one LLM entity is found in the input text.\n",
    "    \n",
    "    Args:\n",
    "        llm_entities (dict): Dictionary of LLM entities.\n",
    "        input_text (str): The original input text.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if at least one entity is found, False otherwise.\n",
    "    \"\"\"\n",
    "    # Check if any LLM entities are present in the input text\n",
    "    # Iterate through the dictionary of LLM entities\n",
    "    # and check if any value is present in the input text\n",
    "    for tag, values in llm_entities.items():\n",
    "        # rint(f\"Checking for tag: {tag} with values: {values}\")\n",
    "        for value in values:\n",
    "            # print(f\"Checking if '{value}' is in input text.\")\n",
    "            if value in input_text:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0a6643",
   "metadata": {},
   "source": [
    "We're going to want to save the results of our quality control, to then manipulate later when we score, or if we want to visulise them through plots and graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b50c06b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_qc_results_to_csv(rows: List[Dict[str, Any]], output_dir: str | Path) -> None:\n",
    "    \"\"\"Append rows to quality_control_results.csv inside output_dir.\n",
    "\n",
    "    Each row dict must have the keys\n",
    "    prompt, model, filename, text_unchanged, one_entity_verified, and full_path.\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    csv_path = output_dir / \"quality_control_results.csv\"\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(csv_path, index=False, mode=\"a\", header=not csv_path.exists())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22572d6f",
   "metadata": {},
   "source": [
    "This is the function responsible for the quality control of the LLM outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0936451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality_control(path_to_test: str | Path) -> None:\n",
    "    \"\"\"Check the quality of the responses and write one summary row per file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path_to_test: str | Path\n",
    "        Path to the folder containing the llm responses.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if quality_control_results.csv already exists\n",
    "    results_file = Path(QC_RESULTS_PATH)\n",
    "    if results_file.exists():\n",
    "        os.remove(results_file)\n",
    "        print(f\"Overwriting existing file: {results_file}\\n\\n\")\n",
    "\n",
    "\n",
    "    path_to_test = Path(path_to_test)\n",
    "\n",
    "    # Collect rows in memory\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    for prompt in os.listdir(path_to_test):  # ─── prompt loop ────────────────────────────\n",
    "        prompt_name = Path(prompt).name\n",
    "        prompt_folder = path_to_test / prompt_name\n",
    "\n",
    "        prompt_total = prompt_conserved = prompt_modified = 0\n",
    "\n",
    "        for model in os.listdir(prompt_folder):  # ─── model loop ───────────────────────────\n",
    "            # If model is a \"meta-llama\" model, go into the directory to get the full model name\n",
    "            # (e.g. \"meta-llama/llama-4-maverick-17b-128e-instruct\")\n",
    "            # Otherwise, use the model name directly\n",
    "            # (e.g. \"gemma2-9b-it\")\n",
    "            if model.startswith(\"meta-llama\"):\n",
    "                model_path = prompt_folder / model # Path\n",
    "                subdirs = [dir.name for dir in model_path.iterdir() if dir.is_dir()]\n",
    "\n",
    "                if len(subdirs) > 1:\n",
    "                    print(f\"Warning: multiple models found in {model_path}\")\n",
    "                else:\n",
    "                    only_model = subdirs[0] # still a string\n",
    "                    model_folder = model_path / only_model  # Path / str → Path\n",
    "                    model = f\"{model}/{only_model}\" # full name for later use\n",
    "            else:\n",
    "                model_folder = prompt_folder / model # Path / str → Path\n",
    "\n",
    "\n",
    "            for filename in os.listdir(model_folder):  # ─ file loop ─────────\n",
    "                prompt_total += 1\n",
    "                file_path = model_folder / filename\n",
    "\n",
    "                # ------------------------------------------------------------------\n",
    "                # Supply your own helpers for these next three calls\n",
    "                # ------------------------------------------------------------------\n",
    "                input_text, response, _ = process_llm_json_file(file_path)\n",
    "                llm_entities = extract_entities_from_llm_text(response)\n",
    "\n",
    "                exact_text_result = compare_annotated_to_original(input_text, response)\n",
    "                entities_result = find_one_valid_llm_entity(llm_entities, input_text)\n",
    "                # ------------------------------------------------------------------\n",
    "\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"prompt\": prompt_name,\n",
    "                        \"model\": model,\n",
    "                        \"filename\": filename,\n",
    "                        \"text_unchanged\": exact_text_result,\n",
    "                        \"one_entity_verified\": entities_result,\n",
    "                        \"full_path\": str(file_path),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    # Persist everything in one write\n",
    "    save_qc_results_to_csv(rows, QC_RESULTS_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2ccd3e",
   "metadata": {},
   "source": [
    "### **Actual quality control and saving results:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4a9060ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_control(LLM_ANNOTATIONS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
