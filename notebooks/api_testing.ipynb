{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "from groq import Groq\n",
    "import json\n",
    "import jinja2\n",
    "import re\n",
    "from typing import Dict, List, Tuple\n",
    "import datetime\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Prompt engineering with LLMs**\n",
    "---\n",
    "\n",
    "Currently in this notebook, we have tested only models using the Groq API.\n",
    "Models from using the OpenAI API will be added soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which API to use\n",
    "def get_api(api_type: str) -> object:\n",
    "    if api_type == \"openai\":\n",
    "        return OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "    elif api_type == \"groq\":\n",
    "        return Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "    else:\n",
    "        raise ValueError(\"Invalid API type. Choose 'openai' or 'groq'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_type = \"openai\"\n",
    "\n",
    "# Set up the environment variable for the API key\n",
    "client = get_api(api_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have:\n",
    "- paths to the prompt template and the annotation files folder\n",
    "\n",
    "\n",
    "- the list of types of prompts (zero-shot, one-shot, few-shot)\n",
    "\n",
    "\n",
    "- the list of models tested\n",
    "\n",
    "\n",
    "- the list of entities in which we are interested in\n",
    "\n",
    "**MODIFY THE LIST OF PROMPTS AND MODELS TO YOUR LIKING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder with the ground-truth texts\n",
    "ANNOTATIONS_FOLDER = \"../annotations/\"\n",
    "\n",
    "# Folder with the prompt templates\n",
    "PROMPT_PATH = \"../prompt_templates/\"\n",
    "\n",
    "# List of prompt templates\n",
    "LIST_PROMPTS = [\n",
    "    \"zero_shot\",\n",
    "    # \"one_shot\",\n",
    "    \"few_shot\"\n",
    "]\n",
    "\n",
    "# List of models to test\n",
    "LIST_MODELS_GROQ = [\n",
    "    # \"gemma2-9b-it\",\n",
    "    # \"mistral-saba-24b\",\n",
    "    \"llama-3.3-70b-versatile\",\n",
    "    \"meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
    "    \"deepseek-r1-distill-llama-70b\"\n",
    "    # \"deepseek-r1-distill-qwen-32b\" # has been decommissioned\n",
    "    ]\n",
    "\n",
    "LIST_MODELS_OPENAI = [\n",
    "    \"gpt-4.1-2025-04-14\",\n",
    "    \"gpt-4o-2024-11-20\",\n",
    "    \"o3-2025-04-16\"\n",
    "]\n",
    "\n",
    "# List of entities to tag (by the llms) and then extract\n",
    "TAGS = [\n",
    "    \"MOL\", \"SOFTNAME\", \"SOFTVERS\", \"STIME\", \"TEMP\", \"FFM\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we need a helper function to extract certain information from the ground-truth data:\n",
    "- The input text that needs to be annotated (`input_text`)\n",
    "\n",
    "\n",
    "- The manually-found entities (`ground_truth_entities`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process one JSON file to extract the ground truth entities and the input text\n",
    "def process_json_file(json_file: str) -> tuple:\n",
    "    with open(json_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Extract the input text\n",
    "    annotation_entry = data[\"annotations\"][0]\n",
    "    input_text = annotation_entry[0]\n",
    "    ground_truth_entities = annotation_entry[1][\"entities\"]\n",
    "\n",
    "    return input_text, ground_truth_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define several helper functions to assist with the annotation workflow:\n",
    "\n",
    "1. **Render the prompt template**  \n",
    "\n",
    "   We use a Jinja2 template to dynamically inject the text that needs to be annotated. This allows for flexible and reusable prompt formatting.\n",
    "\n",
    "2. **Interact with the LLM using the template**  \n",
    "\n",
    "   This function handles communication with the language model using the rendered prompt. It is currently tailored for the Groq API, though the structure may vary if you use a different API.\n",
    "\n",
    "3. **Save the LLM response to a JSON file**  \n",
    "\n",
    "   To maintain a record of the process, we save the model's response along with metadata, including the model used, the prompt sent, and the annotated output (in XML format with entity annotations).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_render_prompt(template_path: str, text_to_annotate: str) -> str:\n",
    "    \"\"\"\n",
    "    Load a Jinja2 template from a file and render it with the provided text to annotate.\n",
    "    \n",
    "    Args:\n",
    "        template_path (str): Path to the template file.\n",
    "        text_to_annotate (str): Text to be annotated.\n",
    "    \n",
    "    Returns:\n",
    "        str: Rendered prompt string.\n",
    "    \"\"\"\n",
    "    with open(template_path, \"r\") as f:\n",
    "        template_content = f.read()\n",
    "    template = jinja2.Template(template_content)\n",
    "    return template.render(text_to_annotate=text_to_annotate)\n",
    "\n",
    "\n",
    "def chat_with_template(prompt:str, template_path: str, model:str, text_to_annotate:str) -> str:\n",
    "    \"\"\"\n",
    "    Chat with the Groq API using a template and a model.\n",
    "    Args:\n",
    "        template_path (str): Path to the template file.\n",
    "        prompt (str): Rendered prompt.\n",
    "        model (str): Model to use for the chat.\n",
    "        text_to_annotate (str): Text to be annotated.\n",
    "    Returns:\n",
    "        str: Response from the chat completion.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = load_and_render_prompt(template_path, text_to_annotate)\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        model=model,\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "\n",
    "def save_response_as_json(response_text:str, output_path:str) -> None:\n",
    "    \"\"\"\n",
    "    Takes the response text from the AI and saves it as a JSON file.\n",
    "    Args:\n",
    "        response_text (str): The response text to save.\n",
    "        output_path (str): Path to the output JSON file.\n",
    "    \"\"\"\n",
    "    response_text\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(response_text, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start running annotations, we need to set up a directory structure to organize the outputs based on prompt types and models used.\n",
    "\n",
    "The following code will:\n",
    "- Create a root folder to store all LLM annotations.\n",
    "\n",
    "\n",
    "- For each prompt template in `LIST_PROMPTS`, create a subfolder.\n",
    "\n",
    "\n",
    "- Within each prompt folder, create additional subfolders for each model in `LIST_MODELS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current date and time\n",
    "date_and_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "if api_type == \"groq\":\n",
    "    LIST_MODELS = LIST_MODELS_GROQ\n",
    "elif api_type == \"openai\":\n",
    "    LIST_MODELS = LIST_MODELS_OPENAI\n",
    "else:\n",
    "    raise ValueError(\"Invalid API type. Choose 'openai' or 'groq'.\")\n",
    "\n",
    "# Make repository for LLM annotations\n",
    "ouput_llm_annotation_folder = f\"../llm_outputs/output_llm_annotations_{date_and_time}/\"\n",
    "if not os.path.exists(ouput_llm_annotation_folder):\n",
    "    os.makedirs(ouput_llm_annotation_folder)\n",
    "\n",
    "# Create folder for each prompt type\n",
    "# Then within that, create a folder for each model\n",
    "for prompt in LIST_PROMPTS:\n",
    "    prompt_name = os.path.basename(prompt)\n",
    "    output_prompt_folder = os.path.join(ouput_llm_annotation_folder, prompt_name)\n",
    "    if not os.path.exists(output_prompt_folder):\n",
    "        os.makedirs(output_prompt_folder)\n",
    "\n",
    "    for model in LIST_MODELS:\n",
    "        output_model_folder = os.path.join(output_prompt_folder, model)\n",
    "        if not os.path.exists(output_model_folder):\n",
    "            os.makedirs(output_model_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Run LLM annotations**\n",
    "---\n",
    "\n",
    "We will now test our LLM annotation pipeline on a subset of input texts. Specifically, we will:\n",
    "\n",
    "- Select the first 10 input files from the annotations folder.\n",
    "\n",
    "    - For each file, we will apply:\n",
    "\n",
    "        - Each prompt template in `LIST_PROMPTS`\n",
    "\n",
    "        - Each language model in `LIST_MODELS`\n",
    "\n",
    "    - Save the LLM's annotated response as a JSON file in a their designated directory: `../output_llm_annotations/{prompt_name}/{model}/{filename}`\n",
    "\n",
    "\n",
    "\n",
    "We can modify the amount of files that are annotated. To give an idea, for two models, three types of prompts, and 10 texts to annotate, this takes **~ 7mins**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file 1: figshare_22213635.json ==============\n",
      "\n",
      "File 1 - Testing prompt: zero_shot -------\n",
      "\n",
      "File 1 - Testing model: gpt-4.1-2025-04-14\n",
      "File 1 - Testing model: gpt-4o-2024-11-20\n",
      "File 1 - Testing model: o3-2025-04-16\n",
      "\n",
      "File 1 - Testing prompt: few_shot -------\n",
      "\n",
      "File 1 - Testing model: gpt-4.1-2025-04-14\n",
      "File 1 - Testing model: gpt-4o-2024-11-20\n"
     ]
    }
   ],
   "source": [
    "# Use 10 input texts from the annotation folder\n",
    "number_texts = 0\n",
    "\n",
    "for filename in os.listdir(ANNOTATIONS_FOLDER): # Loop through the files in the annotations folder\n",
    "    if number_texts >= 10:\n",
    "        break\n",
    "\n",
    "    if filename.endswith(\".json\") and filename.count(\"_\") == 1:\n",
    "        number_texts += 1\n",
    "\n",
    "        print(f\"\\nProcessing file {number_texts}: {filename} ==============\")\n",
    "        input_text, _ = process_json_file(os.path.join(ANNOTATIONS_FOLDER, filename))\n",
    "\n",
    "        for prompt in LIST_PROMPTS: # Testing each type of prompt\n",
    "            print(f\"\\nFile {number_texts} - Testing prompt: {prompt} -------\\n\")\n",
    "\n",
    "            prompt_name = os.path.basename(prompt)\n",
    "            output_prompt_folder = os.path.join(ouput_llm_annotation_folder, prompt_name)\n",
    "\n",
    "            for model in LIST_MODELS: # Testing each model\n",
    "                print(f\"File {number_texts} - Testing model: {model}\")\n",
    "\n",
    "                output_model_folder = os.path.join(output_prompt_folder, model)\n",
    "\n",
    "                response = chat_with_template(\n",
    "                    prompt=prompt,\n",
    "                    template_path=os.path.join(PROMPT_PATH, f\"{prompt}.txt\"),\n",
    "                    model=model,\n",
    "                    text_to_annotate=input_text\n",
    "                )\n",
    "\n",
    "                # Save the response as a JSON file\n",
    "                output_path_for_json = os.path.join(output_prompt_folder, model, filename)\n",
    "                data = {\n",
    "                    \"model\": model,\n",
    "                    \"text_to_annotate\": input_text,\n",
    "                    \"response\": response\n",
    "                }\n",
    "                save_response_as_json(data, output_path_for_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Response quality control**\n",
    "---\n",
    "\n",
    "Before analyzing the LLM responses, we implement a few helper functions to perform a basic quality check. This check helps ensure that the outputs meet a minimum standard before moving forward with evaluation or further processing.\n",
    "\n",
    "For now, our quality control focuses on **verifying that the LLM response includes the original input text**. This simple check helps catch hallucinations or unrelated outputs from the model. Note that the comparison is **case-insensitive**.\n",
    "\n",
    "While this is a strict baseline, it provides a quick filter for obviously flawed responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the future, we plan to implement more nuanced validation criteria, such as  ensuring that entities annotated by the LLM actually appear in the original input text.\n",
    "\n",
    "This will allow us to be more flexible while still maintaining meaningful quality standards.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure the integrity of the LLM annotations, we need to verify that the output text closely matches the input text we provided.\n",
    "\n",
    "1. **Strip annotation tags**\n",
    "\n",
    "\n",
    "Since the LLM response includes XML-like tags (e.g., `<TAG LABEL>` and `</TAG LABEL>`), we first remove these tags to isolate the raw text. This allows for a fair comparison between the original input and the annotated output.\n",
    "\n",
    "2. **Compare input and output texts**\n",
    "\n",
    "\n",
    "Once tags are stripped, we compare the cleaned LLM output to the original input text. This helps us confirm that the model has not introduced hallucinated content or omitted any part of the input. Only the responses that **pass this check** are retained for further analysis or evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to strip tags from the annotated text\n",
    "def strip_tags(text:str, tags=TAGS) -> str:\n",
    "    \"\"\"\n",
    "    Removes the custom tags from the annotated text.\n",
    "    \"\"\"\n",
    "    for tag in tags:\n",
    "        text = re.sub(f\"</?{re.escape(tag)}>\", \"\", text)\n",
    "    return text.strip()\n",
    "\n",
    "# Function to compare the annotated text to the original input\n",
    "def compare_annotated_to_original(original: str, annotated: str) -> bool:\n",
    "    \"\"\"\n",
    "    Compares tag-stripped annotated text to the original input in lowercase.\n",
    "    Returns True if they match exactly (ignoring case), False otherwise.\n",
    "    \"\"\"\n",
    "    stripped = strip_tags(annotated).strip().lower()\n",
    "    original = original.strip().lower()\n",
    "\n",
    "    return stripped == original\n",
    "\n",
    "def load_annotation_result(file_path:str) -> tuple:\n",
    "    \"\"\"\n",
    "    Load the annotation result from a JSON file.\n",
    "    Args:\n",
    "        file_path (str): Path to the JSON file.\n",
    "    Returns:\n",
    "        tuple: Original text and annotated output.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        original_text = data.get(\"text_to_annotate\")\n",
    "        annotated_output = data.get(\"response\")\n",
    "\n",
    "        if original_text is None or annotated_output is None:\n",
    "            raise ValueError(\"Missing required fields in JSON: 'text_to_annotate' or 'response'.\")\n",
    "\n",
    "        return original_text, annotated_output\n",
    "\n",
    "    except (json.JSONDecodeError, FileNotFoundError, ValueError) as e:\n",
    "        print(f\"Error loading file {file_path}:\", e)\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">\n",
    ">Later on: Qualifying rejected annotations\n",
    ">\n",
    ">Although not implemented yet, we aim to better understand and categorize the annotations that fail quality control. These might include:\n",
    ">\n",
    ">- **Text changed, but entities preserved**  \n",
    ">\n",
    ">- **Entities altered or incorrectly annotated**  \n",
    ">\n",
    ">- **Significant rewrites of the input (both entities and text)**\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section evaluates whether the annotated outputs generated by the LLMs preserve the original input text (after stripping tags). We track and record whether each annotated file was conserved or modified.\n",
    "At the end of each prompt's processing loop, we print:\n",
    "- Total number of texts processed\n",
    "\n",
    "- Number of conserved outputs\n",
    "\n",
    "- Number of modified outputs\n",
    "\n",
    "\n",
    "This is based on **the type of prompt** (zero-shot, one-shot, and few-shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt type: zero_shot ==============\n",
      "\n",
      "Model ID: gpt-4.1-2025-04-14\n",
      "\n",
      "Processing file : figshare_22213635.json ---------\n",
      "Original text conserved? - False \n",
      "\n",
      "Processing file : figshare_4757161.json ---------\n",
      "Original text conserved? - False \n",
      "\n",
      "Processing file : figshare_21263177.json ---------\n",
      "Original text conserved? - False \n",
      "\n",
      "Processing file : zenodo_6582985.json ---------\n",
      "Original text conserved? - False \n",
      "\n",
      "Processing file : zenodo_6478270.json ---------\n",
      "Original text conserved? - False \n",
      "\n",
      "Processing file : figshare_7783568.json ---------\n",
      "Original text conserved? - False \n",
      "\n",
      "Processing file : figshare_20300547.json ---------\n",
      "Original text conserved? - False \n",
      "\n",
      "Processing file : zenodo_4805388.json ---------\n",
      "Original text conserved? - True \n",
      "\n",
      "Processing file : zenodo_51754.json ---------\n",
      "Original text conserved? - False \n",
      "\n",
      "Processing file : figshare_20009556.json ---------\n",
      "Original text conserved? - False \n",
      "\n",
      "\n",
      "Results for prompt: zero_shot\n",
      "Total texts processed: 10\n",
      "Conserved texts: 1\n",
      "Modified texts: 9\n",
      "\n",
      "Prompt type: few_shot ==============\n",
      "\n",
      "Model ID: gpt-4.1-2025-04-14\n",
      "\n",
      "Processing file : figshare_22213635.json ---------\n",
      "Original text conserved? - False \n",
      "\n",
      "Processing file : figshare_4757161.json ---------\n",
      "Original text conserved? - False \n",
      "\n",
      "Processing file : figshare_21263177.json ---------\n",
      "Original text conserved? - False \n",
      "\n",
      "Processing file : zenodo_6582985.json ---------\n",
      "Original text conserved? - True \n",
      "\n",
      "Processing file : zenodo_6478270.json ---------\n",
      "Original text conserved? - True \n",
      "\n",
      "Processing file : figshare_7783568.json ---------\n",
      "Original text conserved? - True \n",
      "\n",
      "Processing file : figshare_20300547.json ---------\n",
      "Original text conserved? - True \n",
      "\n",
      "Processing file : zenodo_4805388.json ---------\n",
      "Original text conserved? - False \n",
      "\n",
      "Processing file : zenodo_51754.json ---------\n",
      "Original text conserved? - True \n",
      "\n",
      "Processing file : figshare_20009556.json ---------\n",
      "Original text conserved? - False \n",
      "\n",
      "\n",
      "Results for prompt: few_shot\n",
      "Total texts processed: 10\n",
      "Conserved texts: 5\n",
      "Modified texts: 5\n"
     ]
    }
   ],
   "source": [
    "list_of_conserved_llm_texts = []\n",
    "list_of_conserved_filenames = []\n",
    "list_of_modified_filenames = []\n",
    "list_of_modified_llm_texts = []\n",
    "\n",
    "for prompt in LIST_PROMPTS: # Testing each type of prompt\n",
    "    print(f\"\\nPrompt type: {prompt} ==============\\n\")\n",
    "\n",
    "    prompt_name = os.path.basename(prompt)\n",
    "    output_prompt_folder = os.path.join(ouput_llm_annotation_folder, prompt_name)\n",
    "\n",
    "    # Counter for the number of texts for each prompt\n",
    "    prompt_total_texts = 0\n",
    "    prompt_conserved_texts = 0\n",
    "    prompt_modified_texts = 0\n",
    "\n",
    "    for model in LIST_MODELS: # Testing each model\n",
    "        print(f\"Model ID: {model}\\n\")\n",
    "\n",
    "        output_model_folder = os.path.join(output_prompt_folder, model)\n",
    "\n",
    "        for filename in os.listdir(output_model_folder): # Loop through the files in the annotations folder\n",
    "            print(f\"Processing file : {filename} ---------\")\n",
    "            input_text, ground_truth_entities = process_json_file(os.path.join(ANNOTATIONS_FOLDER, filename))\n",
    "\n",
    "            # Load the annotation result\n",
    "            original_text, annotated_output = load_annotation_result(os.path.join(output_model_folder, filename))\n",
    "\n",
    "            # Compare the original text to the annotated output\n",
    "            result = compare_annotated_to_original(original_text, annotated_output)\n",
    "            print(\"Original text conserved? -\", result, '\\n')\n",
    "            if not result:\n",
    "                list_of_modified_llm_texts.append(os.path.join(output_model_folder, filename))\n",
    "                list_of_modified_filenames.append(os.path.join(ANNOTATIONS_FOLDER, filename))\n",
    "            else:\n",
    "                list_of_conserved_llm_texts.append(os.path.join(output_model_folder, filename))\n",
    "                list_of_conserved_filenames.append(os.path.join(ANNOTATIONS_FOLDER, filename))\n",
    "    \n",
    "            # Update the prompt-specific counters\n",
    "            prompt_total_texts += 1\n",
    "            if result:\n",
    "                prompt_conserved_texts += 1\n",
    "            else:\n",
    "                prompt_modified_texts += 1\n",
    "\n",
    "    # Print the results for each prompt\n",
    "    print(\"\\nResults for prompt:\", prompt)\n",
    "    print(f\"Total texts processed: {prompt_total_texts}\")\n",
    "    print(f\"Conserved texts: {prompt_conserved_texts}\")\n",
    "    print(f\"Modified texts: {prompt_modified_texts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we aggregate quality control results per **model**, allowing us to assess how well each model preserves the original input text across all prompt types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model ID: gpt-4.1-2025-04-14\n",
      "\n",
      "Prompt type: zero_shot ==============\n",
      "\n",
      "Processing file : figshare_22213635.json ---------\n",
      "Original text conserved? - False\n",
      "\n",
      "Processing file : figshare_4757161.json ---------\n",
      "Original text conserved? - False\n",
      "\n",
      "Processing file : figshare_21263177.json ---------\n",
      "Original text conserved? - False\n",
      "\n",
      "Processing file : zenodo_6582985.json ---------\n",
      "Original text conserved? - False\n",
      "\n",
      "Processing file : zenodo_6478270.json ---------\n",
      "Original text conserved? - False\n",
      "\n",
      "Processing file : figshare_7783568.json ---------\n",
      "Original text conserved? - False\n",
      "\n",
      "Processing file : figshare_20300547.json ---------\n",
      "Original text conserved? - False\n",
      "\n",
      "Processing file : zenodo_4805388.json ---------\n",
      "Original text conserved? - True\n",
      "\n",
      "Processing file : zenodo_51754.json ---------\n",
      "Original text conserved? - False\n",
      "\n",
      "Processing file : figshare_20009556.json ---------\n",
      "Original text conserved? - False\n",
      "\n",
      "Prompt type: few_shot ==============\n",
      "\n",
      "Processing file : figshare_22213635.json ---------\n",
      "Original text conserved? - False\n",
      "\n",
      "Processing file : figshare_4757161.json ---------\n",
      "Original text conserved? - False\n",
      "\n",
      "Processing file : figshare_21263177.json ---------\n",
      "Original text conserved? - False\n",
      "\n",
      "Processing file : zenodo_6582985.json ---------\n",
      "Original text conserved? - True\n",
      "\n",
      "Processing file : zenodo_6478270.json ---------\n",
      "Original text conserved? - True\n",
      "\n",
      "Processing file : figshare_7783568.json ---------\n",
      "Original text conserved? - True\n",
      "\n",
      "Processing file : figshare_20300547.json ---------\n",
      "Original text conserved? - True\n",
      "\n",
      "Processing file : zenodo_4805388.json ---------\n",
      "Original text conserved? - False\n",
      "\n",
      "Processing file : zenodo_51754.json ---------\n",
      "Original text conserved? - True\n",
      "\n",
      "Processing file : figshare_20009556.json ---------\n",
      "Original text conserved? - False\n",
      "\n",
      "Results for model: gpt-4.1-2025-04-14\n",
      "Total texts processed: 20\n",
      "Conserved texts: 6\n",
      "Modified texts: 14\n"
     ]
    }
   ],
   "source": [
    "for model in LIST_MODELS: # Testing each model\n",
    "    print(f\"\\nModel ID: {model}\")\n",
    "\n",
    "    model_total_texts = 0\n",
    "    model_conserved_total_texts = 0\n",
    "    model_modified_total_texts = 0\n",
    "\n",
    "    for prompt in LIST_PROMPTS:\n",
    "        print(f\"\\nPrompt type: {prompt} ==============\")\n",
    "        prompt_name = os.path.basename(prompt)\n",
    "        output_prompt_folder = os.path.join(ouput_llm_annotation_folder, prompt_name)\n",
    "        output_model_folder = os.path.join(output_prompt_folder, model)\n",
    "\n",
    "        # Loop through the files in the folder\n",
    "        for filename in os.listdir(output_model_folder):\n",
    "            print(f\"\\nProcessing file : {filename} ---------\")\n",
    "            input_text, ground_truth_entities = process_json_file(os.path.join(ANNOTATIONS_FOLDER, filename))\n",
    "            # Load the annotation result\n",
    "            original_text, annotated_output = load_annotation_result(os.path.join(output_model_folder, filename))\n",
    "            # Compare the original text to the annotated output\n",
    "            result = compare_annotated_to_original(original_text, annotated_output)\n",
    "            print(\"Original text conserved? -\", result)\n",
    "            if not result:\n",
    "                model_modified_total_texts += 1\n",
    "            else:\n",
    "                model_conserved_total_texts += 1\n",
    "            model_total_texts += 1\n",
    "\n",
    "    print(\"\\nResults for model:\", model)\n",
    "    print(f\"Total texts processed: {model_total_texts}\")\n",
    "    print(f\"Conserved texts: {model_conserved_total_texts}\")\n",
    "    print(f\"Modified texts: {model_modified_total_texts}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LLM annotations scoring**\n",
    "---\n",
    "\n",
    "To assess the quality of entity annotations produced by different LLMs, we implement a set of evaluation metrics that allow both quantitative and qualitative analysis. We aim to measure how well each model performs in identifying and labeling entities.\n",
    "\n",
    "However, before we can properly assess the quality of the annotations, we need to extarct the entities and store them in a standard structure.\n",
    "Both the ground-truth entities and the llm-annotated entities will be in the following structure:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"MOL\": [\"arylamide\", \"hDM2\", \"p53\", \"Nutlin-2\", \"benzodiazepinedione\"],\n",
    "  \"SOFTNAME\": [\"AutoDock\"],\n",
    "  \"SOFTVERS\": [],\n",
    "  \"STIME\": [\"20 ns\"],\n",
    "  \"TEMP\": [],\n",
    "  \"FFM\": [\"GAFF\"]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this, we need different helper functions that will: :\n",
    "\n",
    "- convert the current **ground-truth annotation** format to the one we want\n",
    "\n",
    "\n",
    "Current ground-truth annotation format:\n",
    "```json\n",
    "{\n",
    "  \"classes\": [\"TEMP\", \"SOFT\", \"STIME\", \"MOL\", \"FFM\"],\n",
    "  \"annotations\": [[\n",
    "      \"An in silico approach to determine inter-subunit affinities in human septin complexes.\",\n",
    "      {\"entities\": [[69, 75, \"MOL\"], [90, 97, \"MOL\"], [1255, 1260, \"MOL\"], [1368, 1374, \"MOL\"]]}\n",
    "  ]]\n",
    "}\n",
    "```\n",
    "\n",
    "- convert the **llm-ouput annotation** format to the one we want\n",
    "\n",
    "llm-ouput annotation format:\n",
    "```json\n",
    "{\n",
    "  \"model\": \"gemma2-9b-it\",\n",
    "  \"text_to_annotate\": \"Extending the Stochastic Titration CpHMD to CHARMM36m.\",\n",
    "  \"response\": \"Extending the Stochastic Titration CpHMD to <FFM>CHARMM36m</FFM>.\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_llm_json_file(json_file: str) -> tuple:\n",
    "    with open(json_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Extract the input text, response, and model\n",
    "    text_to_annotate = data[\"text_to_annotate\"]\n",
    "    response = data[\"response\"]\n",
    "    model = data[\"model\"]\n",
    "\n",
    "    return text_to_annotate, response, model\n",
    "\n",
    "def extract_entities_from_llm_text(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extract entities from an output text based on tagged annotations.\n",
    "    \n",
    "    The input text is expected to have entities enclosed in tags, e.g.:\n",
    "    \"Extending the Stochastic Titration CpHMD to <FFM>CHARMM36m</FFM> using <SOFTNAME>Gromacs</SOFTNAME>\"\n",
    "    \n",
    "    The function returns a dictionary with keys corresponding to the desired entity types\n",
    "    and values as lists with the extracted entity content.\n",
    "    \"\"\"\n",
    "    # Initialize the results with empty lists for all desired keys.\n",
    "    result = {key: [] for key in TAGS}\n",
    "    \n",
    "    # Use a regex to capture tags in the format <TAG>content</TAG>\n",
    "    # The regex uses a backreference to ensure matching closing tag.\n",
    "    pattern = re.compile(r\"<([A-Z]+)>(.*?)</\\1>\")\n",
    "    \n",
    "    # Find all matches in the text.\n",
    "    for tag, content in pattern.findall(text):\n",
    "        # If the tag is one of our desired keys, append the content (stripped of whitespace)\n",
    "        if tag in result:\n",
    "            result[tag].append(content.strip())\n",
    "    \n",
    "    return result\n",
    "\n",
    "def extract_entities_from_annotation(text: str, entities: list) -> dict:\n",
    "    \"\"\"\n",
    "    Extract entities from the given text based on a direct list of annotation triples.\n",
    "\n",
    "    The entities input should be a list of lists formatted as:\n",
    "    [\n",
    "        [start_index, end_index, \"ENTITY_TYPE\"],\n",
    "        ...\n",
    "    ]\n",
    "    \n",
    "    The function extracts the substring from the text using the provided character indices\n",
    "    and groups the results by the entity type according to TAGS.\n",
    "    If an entity type is not in TAGS, it will be ignored.\n",
    "    If no entities are found for a type, its output list will remain empty.\n",
    "    \n",
    "    The function returns a dictionary with keys corresponding to the desired entity types\n",
    "    and values as lists with the extracted entity content.\n",
    "    \"\"\"\n",
    "    # Initialize the output dictionary with empty lists for each desired key.\n",
    "    result = {key: [] for key in TAGS}\n",
    "    \n",
    "    # Iterate over each entity annotation.\n",
    "    for start, end, entity_type in entities:\n",
    "        if entity_type == 'SOFT':\n",
    "            entity_type = 'SOFTNAME'\n",
    "\n",
    "        extracted = text[start:end]\n",
    "        result[entity_type].append(extracted)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now onto the scoring. **Evaluation logic:**\n",
    "\n",
    "\n",
    "\n",
    "1. **Exact match scoring**: Entity is correct if string and type match exactly.\n",
    "\n",
    "\n",
    "2. **Confidence score**: Fraction of LLMs that agreed on the same entity. (!!! tricky because not all the llms will conserve the text) - ***Not added in yet***\n",
    "\n",
    "\n",
    "3. **Detection ratio**: Correct entities found vs. total ground truth.\n",
    "\n",
    "\n",
    "4. **False positives**: Entities predicted but not in ground truth.\n",
    "\n",
    "\n",
    "5. **False negatives**: Ground truth entities missed by LLM.\n",
    "\n",
    "\n",
    "6. **Per-type breakdown**: Scores computed by entity type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the exact match score\n",
    "def exact_match_score(ground_truth: Dict[str, List[str]], predicted: Dict[str, List[str]]) -> Tuple[int, int, float]:\n",
    "    \"\"\"\n",
    "    Computes the exact match score across all types.\n",
    "    \n",
    "    - An entity is an exact match if both its string and type match.\n",
    "    - Returns a tuple of (matched_count, total_ground_truth_count, ratio).\n",
    "    \n",
    "    Parameters:\n",
    "        ground_truth (dict): Ground truth annotations.\n",
    "        predicted (dict): Predicted annotations.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (number of exact matches, total ground truth entities, score ratio)\n",
    "    \"\"\"\n",
    "    matched = 0\n",
    "    total = 0\n",
    "    # print(ground_truth.items())\n",
    "    for entity_type, gt_entities in ground_truth.items():\n",
    "        \n",
    "        total += len(gt_entities)\n",
    "        pred_entities = set(predicted.get(entity_type, []))\n",
    "        \n",
    "        # Count only those ground truth entities that appear exactly in the predictions.\n",
    "        for entity in gt_entities:\n",
    "            if entity in pred_entities:\n",
    "                matched += 1\n",
    "                \n",
    "    score_ratio = matched / total if total > 0 else 0\n",
    "    return matched, total, score_ratio\n",
    "\n",
    "\n",
    "def detection_ratio(ground_truth: Dict[str, List[str]], predicted: Dict[str, List[str]]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Computes the detection ratio per entity type.\n",
    "    \n",
    "    - For each entity type, computes the fraction of ground truth entities that were found in the predicted entities.\n",
    "    \n",
    "    Parameters:\n",
    "        ground_truth (dict): Ground truth annotations.\n",
    "        predicted (dict): Predicted annotations.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Mapping from entity type to detection ratio (0 to 1).\n",
    "    \"\"\"\n",
    "    ratios = {}\n",
    "    for entity_type, gt_entities in ground_truth.items():\n",
    "        pred_entities = set(predicted.get(entity_type, []))\n",
    "        if gt_entities:\n",
    "            detected = sum(1 for entity in gt_entities if entity in pred_entities)\n",
    "            ratios[entity_type] = detected / len(gt_entities)\n",
    "        else:\n",
    "            ratios[entity_type] = None  # Undefined (or could be set to 0) if no ground truth for the type.\n",
    "    return ratios\n",
    "\n",
    "\n",
    "def false_positives(ground_truth: Dict[str, List[str]], predicted: Dict[str, List[str]]) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Computes false positive entities per entiy type.\n",
    "    \n",
    "    - False positive: An entity predicted that is not present in the corresponding ground truth.\n",
    "    \n",
    "    Parameters:\n",
    "        ground_truth (dict): Ground truth annotations.\n",
    "        predicted (dict): Predicted annotations.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Mapping from entity type to a list of false positive entities.\n",
    "    \"\"\"\n",
    "    false_positives = {}\n",
    "    for entity_type, pred_entities in predicted.items():\n",
    "        gt_entities = set(ground_truth.get(entity_type, []))\n",
    "        # Any predicted entity not in ground truth is a false positive.\n",
    "        false_positives[entity_type] = [entity for entity in pred_entities if entity not in gt_entities]\n",
    "    return false_positives\n",
    "\n",
    "\n",
    "def false_negatives(ground_truth: Dict[str, List[str]], predicted: Dict[str, List[str]]) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Computes false negative entities per entity type.\n",
    "    \n",
    "    - False negative: A ground truth entity that was missed by prediction.\n",
    "    \n",
    "    Parameters:\n",
    "        ground_truth (dict): Ground truth annotations.\n",
    "        predicted (dict): Predicted annotations.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Mapping from entity type to a list of false negative entities.\n",
    "    \"\"\"\n",
    "    false_negatives = {}\n",
    "    for entity_type, gt_entities in ground_truth.items():\n",
    "        pred_entities = set(predicted.get(entity_type, []))\n",
    "        # Any ground truth entity not found in predictions is a false negative.\n",
    "        false_negatives[entity_type] = [entity for entity in gt_entities if entity not in pred_entities]\n",
    "    return false_negatives\n",
    "\n",
    "\n",
    "def per_type_breakdown(ground_truth: Dict[str, List[str]], predicted: Dict[str, List[str]]) -> Dict[str, Dict[str, any]]:\n",
    "    \"\"\"\n",
    "    Provides a detailed breakdown per entity type.\n",
    "    \n",
    "    For each entity type, returns a dict with:\n",
    "      - 'exact_matches': number of exact matches,\n",
    "      - 'total_gt': total number of ground truth entities,\n",
    "      - 'detection_ratio': fraction of ground truth detected,\n",
    "      - 'false_positives': list of false positive entities,\n",
    "      - 'false_negatives': list of false negative entities.\n",
    "    \n",
    "    Parameters:\n",
    "        ground_truth (dict): Ground truth annotations.\n",
    "        predicted (dict): Predicted annotations.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Breakdown per entity type.\n",
    "    \"\"\"\n",
    "    breakdown = {}\n",
    "    for entity_type in set(ground_truth.keys()).union(set(predicted.keys())):\n",
    "        gt_entities = ground_truth.get(entity_type, [])\n",
    "        pred_entities = predicted.get(entity_type, [])\n",
    "        gt_set = set(gt_entities)\n",
    "        pred_set = set(pred_entities)\n",
    "        \n",
    "        exact_match_count = sum(1 for e in gt_entities if e in pred_set)\n",
    "        total_gt = len(gt_entities)\n",
    "        detection = exact_match_count / total_gt if total_gt > 0 else None\n",
    "        \n",
    "        breakdown[entity_type] = {\n",
    "            'exact_matches': exact_match_count,\n",
    "            'total_gt': total_gt,\n",
    "            'detection_ratio': detection,\n",
    "            'false_positives': len([e for e in pred_entities if e not in gt_set]),\n",
    "            'false_negatives': len([e for e in gt_entities if e not in pred_set])\n",
    "        }\n",
    "        \n",
    "    return breakdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(list_of_conserved_llm_texts)):\n",
    "#     print(\"LLM text path:\", list_of_conserved_llm_texts[i])\n",
    "#     print(\"Ground truth text path:\", list_of_conserved_filenames[i],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../llm_outputs/output_llm_annotations_2025-04-17_17-37-18/zero_shot/gpt-4.1-2025-04-14/zenodo_4805388.json \n",
      "\n",
      "★ Exact match score: 1/6 (0.17)\n",
      "\n",
      "★ False positives (hallucination ?):\n",
      "  MOL: ['TCR-pHLA', 'TCR-pHLA complexes', 'proteins', 'waters']\n",
      "  SOFTNAME: ['amber', 'R', 'R']\n",
      "  SOFTVERS: ['14']\n",
      "  STIME: []\n",
      "  TEMP: []\n",
      "  FFM: []\n",
      "\n",
      "★ False negatives (missed ?):\n",
      "  MOL: ['TCR', 'pHLA', 'TCR', 'pHLA']\n",
      "  SOFTNAME: []\n",
      "  SOFTVERS: []\n",
      "  STIME: []\n",
      "  TEMP: []\n",
      "  FFM: ['amber14']\n",
      "\n",
      "★ Detection ratio per type (# of correct entities found by LLM ÷ # of entities in the ground truth):\n",
      "  MOL: 0.0\n",
      "  SOFTNAME: 1.0\n",
      "  SOFTVERS: None\n",
      "  STIME: None\n",
      "  TEMP: None\n",
      "  FFM: 0.0\n",
      "\n",
      "★ Per-type breakdown:\n",
      "  TEMP: {'exact_matches': 0, 'total_gt': 0, 'detection_ratio': None, 'false_positives': 0, 'false_negatives': 0}\n",
      "  STIME: {'exact_matches': 0, 'total_gt': 0, 'detection_ratio': None, 'false_positives': 0, 'false_negatives': 0}\n",
      "  SOFTNAME: {'exact_matches': 1, 'total_gt': 1, 'detection_ratio': 1.0, 'false_positives': 3, 'false_negatives': 0}\n",
      "  MOL: {'exact_matches': 0, 'total_gt': 4, 'detection_ratio': 0.0, 'false_positives': 4, 'false_negatives': 4}\n",
      "  SOFTVERS: {'exact_matches': 0, 'total_gt': 0, 'detection_ratio': None, 'false_positives': 1, 'false_negatives': 0}\n",
      "  FFM: {'exact_matches': 0, 'total_gt': 1, 'detection_ratio': 0.0, 'false_positives': 0, 'false_negatives': 1}\n",
      "\n",
      " ======================================================================================================================================================================================================== \n",
      "\n",
      "../llm_outputs/output_llm_annotations_2025-04-17_17-37-18/few_shot/gpt-4.1-2025-04-14/zenodo_6582985.json \n",
      "\n",
      "★ Exact match score: 8/13 (0.62)\n",
      "\n",
      "★ False positives (hallucination ?):\n",
      "  MOL: ['POPC (1-palmitoyl-2-oleoyl-phosphatidylcholine)']\n",
      "  SOFTNAME: ['Desmond']\n",
      "  SOFTVERS: ['2019-4']\n",
      "  STIME: ['500ns', '500ns', '500ns']\n",
      "  TEMP: []\n",
      "  FFM: []\n",
      "\n",
      "★ False negatives (missed ?):\n",
      "  MOL: ['1-palmitoyl-2-oleoyl-phosphatidylcholine', 'waters']\n",
      "  SOFTNAME: ['Desmond 2019-4', 'desmond', 'desmond']\n",
      "  SOFTVERS: []\n",
      "  STIME: []\n",
      "  TEMP: []\n",
      "  FFM: []\n",
      "\n",
      "★ Detection ratio per type (# of correct entities found by LLM ÷ # of entities in the ground truth):\n",
      "  MOL: 0.6666666666666666\n",
      "  SOFTNAME: 0.25\n",
      "  SOFTVERS: None\n",
      "  STIME: 1.0\n",
      "  TEMP: None\n",
      "  FFM: 1.0\n",
      "\n",
      "★ Per-type breakdown:\n",
      "  TEMP: {'exact_matches': 0, 'total_gt': 0, 'detection_ratio': None, 'false_positives': 0, 'false_negatives': 0}\n",
      "  STIME: {'exact_matches': 1, 'total_gt': 1, 'detection_ratio': 1.0, 'false_positives': 3, 'false_negatives': 0}\n",
      "  SOFTNAME: {'exact_matches': 1, 'total_gt': 4, 'detection_ratio': 0.25, 'false_positives': 1, 'false_negatives': 3}\n",
      "  MOL: {'exact_matches': 4, 'total_gt': 6, 'detection_ratio': 0.6666666666666666, 'false_positives': 1, 'false_negatives': 2}\n",
      "  SOFTVERS: {'exact_matches': 0, 'total_gt': 0, 'detection_ratio': None, 'false_positives': 1, 'false_negatives': 0}\n",
      "  FFM: {'exact_matches': 2, 'total_gt': 2, 'detection_ratio': 1.0, 'false_positives': 0, 'false_negatives': 0}\n",
      "\n",
      " ======================================================================================================================================================================================================== \n",
      "\n",
      "../llm_outputs/output_llm_annotations_2025-04-17_17-37-18/few_shot/gpt-4.1-2025-04-14/zenodo_6478270.json \n",
      "\n",
      "★ Exact match score: 4/9 (0.44)\n",
      "\n",
      "★ False positives (hallucination ?):\n",
      "  MOL: ['PDZ domain of LNX2 protein', 'protein chains']\n",
      "  SOFTNAME: []\n",
      "  SOFTVERS: []\n",
      "  STIME: []\n",
      "  TEMP: []\n",
      "  FFM: ['CHARMM-modified TIP3P']\n",
      "\n",
      "★ False negatives (missed ?):\n",
      "  MOL: ['LNX2', 'Water', '5E11']\n",
      "  SOFTNAME: []\n",
      "  SOFTVERS: []\n",
      "  STIME: []\n",
      "  TEMP: []\n",
      "  FFM: ['CHARMM', 'TIP3P']\n",
      "\n",
      "★ Detection ratio per type (# of correct entities found by LLM ÷ # of entities in the ground truth):\n",
      "  MOL: 0.4\n",
      "  SOFTNAME: None\n",
      "  SOFTVERS: None\n",
      "  STIME: 1.0\n",
      "  TEMP: None\n",
      "  FFM: 0.3333333333333333\n",
      "\n",
      "★ Per-type breakdown:\n",
      "  TEMP: {'exact_matches': 0, 'total_gt': 0, 'detection_ratio': None, 'false_positives': 0, 'false_negatives': 0}\n",
      "  STIME: {'exact_matches': 1, 'total_gt': 1, 'detection_ratio': 1.0, 'false_positives': 0, 'false_negatives': 0}\n",
      "  SOFTNAME: {'exact_matches': 0, 'total_gt': 0, 'detection_ratio': None, 'false_positives': 0, 'false_negatives': 0}\n",
      "  MOL: {'exact_matches': 2, 'total_gt': 5, 'detection_ratio': 0.4, 'false_positives': 2, 'false_negatives': 3}\n",
      "  SOFTVERS: {'exact_matches': 0, 'total_gt': 0, 'detection_ratio': None, 'false_positives': 0, 'false_negatives': 0}\n",
      "  FFM: {'exact_matches': 1, 'total_gt': 3, 'detection_ratio': 0.3333333333333333, 'false_positives': 1, 'false_negatives': 2}\n",
      "\n",
      " ======================================================================================================================================================================================================== \n",
      "\n",
      "../llm_outputs/output_llm_annotations_2025-04-17_17-37-18/few_shot/gpt-4.1-2025-04-14/figshare_7783568.json \n",
      "\n",
      "★ Exact match score: 3/5 (0.60)\n",
      "\n",
      "★ False positives (hallucination ?):\n",
      "  MOL: ['Bacteriophytochrome', 'biliverdin IX (BV)', 'bacteriophytochrome from Deinococcus radiodurans (DrBphP)', 'DrBphP']\n",
      "  SOFTNAME: ['XMCQDPT2']\n",
      "  SOFTVERS: []\n",
      "  STIME: []\n",
      "  TEMP: []\n",
      "  FFM: []\n",
      "\n",
      "★ False negatives (missed ?):\n",
      "  MOL: ['tetrapyrrole chromophore', 'biliverdin IX']\n",
      "  SOFTNAME: []\n",
      "  SOFTVERS: []\n",
      "  STIME: []\n",
      "  TEMP: []\n",
      "  FFM: []\n",
      "\n",
      "★ Detection ratio per type (# of correct entities found by LLM ÷ # of entities in the ground truth):\n",
      "  MOL: 0.6\n",
      "  SOFTNAME: None\n",
      "  SOFTVERS: None\n",
      "  STIME: None\n",
      "  TEMP: None\n",
      "  FFM: None\n",
      "\n",
      "★ Per-type breakdown:\n",
      "  TEMP: {'exact_matches': 0, 'total_gt': 0, 'detection_ratio': None, 'false_positives': 0, 'false_negatives': 0}\n",
      "  STIME: {'exact_matches': 0, 'total_gt': 0, 'detection_ratio': None, 'false_positives': 0, 'false_negatives': 0}\n",
      "  SOFTNAME: {'exact_matches': 0, 'total_gt': 0, 'detection_ratio': None, 'false_positives': 1, 'false_negatives': 0}\n",
      "  MOL: {'exact_matches': 3, 'total_gt': 5, 'detection_ratio': 0.6, 'false_positives': 4, 'false_negatives': 2}\n",
      "  SOFTVERS: {'exact_matches': 0, 'total_gt': 0, 'detection_ratio': None, 'false_positives': 0, 'false_negatives': 0}\n",
      "  FFM: {'exact_matches': 0, 'total_gt': 0, 'detection_ratio': None, 'false_positives': 0, 'false_negatives': 0}\n",
      "\n",
      " ======================================================================================================================================================================================================== \n",
      "\n",
      "../llm_outputs/output_llm_annotations_2025-04-17_17-37-18/few_shot/gpt-4.1-2025-04-14/figshare_20300547.json \n",
      "\n",
      "★ Exact match score: 4/10 (0.40)\n",
      "\n",
      "★ False positives (hallucination ?):\n",
      "  MOL: ['alkylated graphene oxides (AGOs)', 'AGO sheets', 'alkyl groups', 'AGO-based materials', 'alkyl chains', 'alkyl chain length']\n",
      "  SOFTNAME: ['grand canonical Monte Carlo', 'MD simulation']\n",
      "  SOFTVERS: []\n",
      "  STIME: []\n",
      "  TEMP: []\n",
      "  FFM: []\n",
      "\n",
      "★ False negatives (missed ?):\n",
      "  MOL: ['graphene oxides', 'AGO', 'alkyl', 'AGO', 'alkyl', 'alkyl']\n",
      "  SOFTNAME: []\n",
      "  SOFTVERS: []\n",
      "  STIME: []\n",
      "  TEMP: []\n",
      "  FFM: []\n",
      "\n",
      "★ Detection ratio per type (# of correct entities found by LLM ÷ # of entities in the ground truth):\n",
      "  MOL: 0.4\n",
      "  SOFTNAME: None\n",
      "  SOFTVERS: None\n",
      "  STIME: None\n",
      "  TEMP: None\n",
      "  FFM: None\n",
      "\n",
      "★ Per-type breakdown:\n",
      "  TEMP: {'exact_matches': 0, 'total_gt': 0, 'detection_ratio': None, 'false_positives': 0, 'false_negatives': 0}\n",
      "  STIME: {'exact_matches': 0, 'total_gt': 0, 'detection_ratio': None, 'false_positives': 0, 'false_negatives': 0}\n",
      "  SOFTNAME: {'exact_matches': 0, 'total_gt': 0, 'detection_ratio': None, 'false_positives': 2, 'false_negatives': 0}\n",
      "  MOL: {'exact_matches': 4, 'total_gt': 10, 'detection_ratio': 0.4, 'false_positives': 6, 'false_negatives': 6}\n",
      "  SOFTVERS: {'exact_matches': 0, 'total_gt': 0, 'detection_ratio': None, 'false_positives': 0, 'false_negatives': 0}\n",
      "  FFM: {'exact_matches': 0, 'total_gt': 0, 'detection_ratio': None, 'false_positives': 0, 'false_negatives': 0}\n",
      "\n",
      " ======================================================================================================================================================================================================== \n",
      "\n",
      "../llm_outputs/output_llm_annotations_2025-04-17_17-37-18/few_shot/gpt-4.1-2025-04-14/zenodo_51754.json \n",
      "\n",
      "★ Exact match score: 14/25 (0.56)\n",
      "\n",
      "★ False positives (hallucination ?):\n",
      "  MOL: []\n",
      "  SOFTNAME: ['Gromacs']\n",
      "  SOFTVERS: ['3.x']\n",
      "  STIME: ['11 ns']\n",
      "  TEMP: ['323 K']\n",
      "  FFM: []\n",
      "\n",
      "★ False negatives (missed ?):\n",
      "  MOL: ['cationic lipid', 'dimyristoyltrimethylammoniumpropane', 'zwitterionic DMPC', 'dimyristoylphosphatidylcholine', 'Water', 'water', 'water']\n",
      "  SOFTNAME: ['Gromacs 3.x', 'LINCS', 'SETTLE']\n",
      "  SOFTVERS: []\n",
      "  STIME: []\n",
      "  TEMP: ['323 K.']\n",
      "  FFM: []\n",
      "\n",
      "★ Detection ratio per type (# of correct entities found by LLM ÷ # of entities in the ground truth):\n",
      "  MOL: 0.6111111111111112\n",
      "  SOFTNAME: 0.0\n",
      "  SOFTVERS: None\n",
      "  STIME: 1.0\n",
      "  TEMP: 0.0\n",
      "  FFM: 1.0\n",
      "\n",
      "★ Per-type breakdown:\n",
      "  TEMP: {'exact_matches': 0, 'total_gt': 1, 'detection_ratio': 0.0, 'false_positives': 1, 'false_negatives': 1}\n",
      "  STIME: {'exact_matches': 1, 'total_gt': 1, 'detection_ratio': 1.0, 'false_positives': 1, 'false_negatives': 0}\n",
      "  SOFTNAME: {'exact_matches': 0, 'total_gt': 3, 'detection_ratio': 0.0, 'false_positives': 1, 'false_negatives': 3}\n",
      "  MOL: {'exact_matches': 11, 'total_gt': 18, 'detection_ratio': 0.6111111111111112, 'false_positives': 0, 'false_negatives': 7}\n",
      "  SOFTVERS: {'exact_matches': 0, 'total_gt': 0, 'detection_ratio': None, 'false_positives': 1, 'false_negatives': 0}\n",
      "  FFM: {'exact_matches': 2, 'total_gt': 2, 'detection_ratio': 1.0, 'false_positives': 0, 'false_negatives': 0}\n",
      "\n",
      " ======================================================================================================================================================================================================== \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(list_of_conserved_llm_texts)):\n",
    "    llm_filename = list_of_conserved_llm_texts[i]\n",
    "    gt_filename = list_of_conserved_filenames[i]\n",
    "\n",
    "    # Process the ground-truth JSON file and extract entities\n",
    "    input_text, ground_truth_entities = process_json_file(gt_filename)\n",
    "    gt_extracted = extract_entities_from_annotation(input_text, ground_truth_entities)\n",
    "    # print(\"Ground-truth entities:\", gt_extracted)\n",
    "\n",
    "    # Process the LLM JSON file and extract entities\n",
    "    _, response, _ = process_llm_json_file(llm_filename)\n",
    "    llm_extracted = extract_entities_from_llm_text(response)\n",
    "    # print(\"LLM extracted entities:\", llm_extracted)\n",
    "    \n",
    "\n",
    "    # Calculate the exact match score ========================================\n",
    "    matched, total, score_ratio = exact_match_score(gt_extracted, llm_extracted)\n",
    "    \n",
    "    # False positives ========================================================\n",
    "    fps = false_positives(gt_extracted, llm_extracted)\n",
    "    \n",
    "    # False negatives ========================================================\n",
    "    fns = false_negatives(gt_extracted, llm_extracted)\n",
    "    \n",
    "    # Calculate the detection ratio ==========================================\n",
    "    detect_ratio = detection_ratio(gt_extracted, llm_extracted)\n",
    "\n",
    "    # Per-type breakdown =====================================================\n",
    "    breakdown = per_type_breakdown(gt_extracted, llm_extracted)\n",
    "\n",
    "\n",
    "    # Print results ==========================================================\n",
    "    print(llm_filename, \"\\n\")\n",
    "    print(f\"★ Exact match score: {matched}/{total} ({score_ratio:.2f})\")\n",
    "\n",
    "    print(\"\\n★ False positives (hallucination ?):\")\n",
    "    for etype, fp_list in fps.items():\n",
    "        print(\"  {}: {}\".format(etype, fp_list))\n",
    "\n",
    "    print(\"\\n★ False negatives (missed ?):\")\n",
    "    for etype, fn_list in fns.items():\n",
    "        print(\"  {}: {}\".format(etype, fn_list))\n",
    "\n",
    "    print(\"\\n★ Detection ratio per type (# of correct entities found by LLM ÷ # of entities in the ground truth):\")\n",
    "    for etype, ratio in detect_ratio.items():\n",
    "        print(f\"  {etype}: {ratio}\")\n",
    "\n",
    "    print(\"\\n★ Per-type breakdown:\")\n",
    "    for etype, stats in breakdown.items():\n",
    "        print(\"  {}: {}\".format(etype, stats))\n",
    "    \n",
    "    print(\"\\n\",\"=\"*200, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **WE NEED TO SAVE CERTAIN STATS IN A CSV FILE or DATAFRAME SO THAT WE CAN PLOT SOME GRAPHS OUT OF THEM !!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- QUALITY CONTROL IS TOO STRICT -> Check only entities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
